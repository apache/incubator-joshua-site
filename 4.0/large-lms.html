<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <link rel="stylesheet" type="text/css" media="screen,print" href="../joshua4.css" />
    <title>Joshua | Building large LMs with SRILM</title>
  </head>

  <body>

    <div id="navbar">
      <a href="http://joshua-decoder.org/">
        <img src="../images/joshua-logo-small.png" width="130px" 
             alt="Joshua logo (picture of a Joshua tree)" />
      </a>

      <p class="infobox">
        <b>Stable version</b><br />
        4.1<br/><br/>
        <b>Release date</b><br />
        2013 January
      </p>

<!--       <div class="infobox"> -->
<!--         <b>AUTO LINKS</b><br/> -->
<!--         <ul> -->
<!--            -->
<!--           <li> Advanced features</li> -->
<!--            -->
<!--           <li> Advanced features</li> -->
<!--            -->
<!--           <li> Advanced features</li> -->
<!--            -->
<!--           <li> Building a language pack</li> -->
<!--            -->
<!--           <li> Building a language pack</li> -->
<!--            -->
<!--           <li> Bundling a configuration</li> -->
<!--            -->
<!--           <li> Contributors</li> -->
<!--            -->
<!--           <li> Decoder configuration parameters</li> -->
<!--            -->
<!--           <li> Decoder configuration parameters</li> -->
<!--            -->
<!--           <li> Decoder configuration parameters</li> -->
<!--            -->
<!--           <li> Decoder configuration parameters</li> -->
<!--            -->
<!--           <li> Frequently Asked Questions</li> -->
<!--            -->
<!--           <li> Common problems</li> -->
<!--            -->
<!--           <li> Frequently Asked Questions</li> -->
<!--            -->
<!--           <li> Common problems</li> -->
<!--            -->
<!--           <li> Features</li> -->
<!--            -->
<!--           <li> Features</li> -->
<!--            -->
<!--           <li> Features</li> -->
<!--            -->
<!--           <li> Features</li> -->
<!--            -->
<!--           <li> Joshua file formats</li> -->
<!--            -->
<!--           <li> Joshua file formats</li> -->
<!--            -->
<!--           <li> Joshua file formats</li> -->
<!--            -->
<!--           <li> Joshua file formats</li> -->
<!--            -->
<!--           <li> </li> -->
<!--            -->
<!--           <li> </li> -->
<!--            -->
<!--           <li> </li> -->
<!--            -->
<!--           <li> Fisher and CALLHOME Spanish English Speech Translation Corpus</li> -->
<!--            -->
<!--           <li> Indian Languages Parallel Corpora</li> -->
<!--            -->
<!--           <li> Joshua 4.0 User Documentation</li> -->
<!--            -->
<!--           <li> Language packs</li> -->
<!--            -->
<!--           <li> Paraphrase Packs</li> -->
<!--            -->
<!--           <li> Joshua releases</li> -->
<!--            -->
<!--           <li> Support</li> -->
<!--            -->
<!--           <li> Getting Started</li> -->
<!--            -->
<!--           <li> Welcome to Joshua</li> -->
<!--            -->
<!--           <li> Joshua documentation</li> -->
<!--            -->
<!--           <li> Joshua documentation</li> -->
<!--            -->
<!--           <li> Installation</li> -->
<!--            -->
<!--           <li> Installation</li> -->
<!--            -->
<!--           <li> Alignment with Jacana</li> -->
<!--            -->
<!--           <li> Alignment with Jacana</li> -->
<!--            -->
<!--           <li> Alignment with Jacana</li> -->
<!--            -->
<!--           <li> Building large LMs with SRILM</li> -->
<!--            -->
<!--           <li> Building large LMs with SRILM</li> -->
<!--            -->
<!--           <li> Building large LMs with SRILM</li> -->
<!--            -->
<!--           <li> Building large LMs with SRILM</li> -->
<!--            -->
<!--           <li> Lattice decoding</li> -->
<!--            -->
<!--           <li> Grammar Packing</li> -->
<!--            -->
<!--           <li> Grammar Packing</li> -->
<!--            -->
<!--           <li> Grammar Packing</li> -->
<!--            -->
<!--           <li> Grammar Packing</li> -->
<!--            -->
<!--           <li> The Joshua Pipeline</li> -->
<!--            -->
<!--           <li> The Joshua Pipeline</li> -->
<!--            -->
<!--           <li> The Joshua Pipeline</li> -->
<!--            -->
<!--           <li> The Joshua Pipeline</li> -->
<!--            -->
<!--           <li> Quick Start</li> -->
<!--            -->
<!--           <li> Quick Start</li> -->
<!--            -->
<!--           <li> Releases</li> -->
<!--            -->
<!--           <li> Server mode</li> -->
<!--            -->
<!--           <li> Server mode</li> -->
<!--            -->
<!--           <li> Server mode</li> -->
<!--            -->
<!--           <li> Installing and running the Joshua Decoder</li> -->
<!--            -->
<!--           <li> Grammar extraction with Thrax</li> -->
<!--            -->
<!--           <li> Grammar extraction with Thrax</li> -->
<!--            -->
<!--           <li> Grammar extraction with Thrax</li> -->
<!--            -->
<!--           <li> Grammar extraction with Thrax</li> -->
<!--            -->
<!--           <li> Building Translation Models</li> -->
<!--            -->
<!--           <li> Building Translation Models</li> -->
<!--            -->
<!--           <li> Building Translation Models</li> -->
<!--            -->
<!--           <li> Building Translation Models</li> -->
<!--            -->
<!--           <li> Pipeline tutorial</li> -->
<!--            -->
<!--           <li> Pipeline tutorial</li> -->
<!--            -->
<!--           <li> Pipeline tutorial</li> -->
<!--            -->
<!--           <li> What's New</li> -->
<!--            -->
<!--           <li> What's New</li> -->
<!--            -->
<!--           <li> Z-MERT</li> -->
<!--            -->
<!--           <li> Z-MERT</li> -->
<!--            -->
<!--           <li> Z-MERT</li> -->
<!--            -->
<!--           <li> Z-MERT</li> -->
<!--            -->
<!--           <li> </li> -->
<!--            -->
<!--           <li> </li> -->
<!--            -->
<!--           <li> </li> -->
<!--            -->
<!--         </ul> -->
<!--       </div>   -->

      <div class="infobox">

        <b>Links</b><br />
        <ul>
          <li> <a href="../index.html">Main</a> </li>
          <li> <a href="pipeline.html">Pipeline</a> </li>
          <li> <a href="step-by-step-instructions.html">Manual walkthrough</a> </li>
          <li> <a href="decoder.html">Decoder</a> </li>
          <li> <a href="server.html">Decoder Server</a> </li>
          <li> <a href="file-formats.html">File formats</a> </li>
          <li> <a href="thrax.html">Grammar Extraction</a> </li>
          <li> <a href="../releases.html">Releases</a> </li>
        </ul>
      </div>

      <div class="infobox">
        <b>Advanced</b><br />
        <ul>
<!--          <li> <a href="packing.html">Grammar packing</a> </li> -->
          <li> <a href="large-lms.html">Building large LMs</a> </li>
          <li> <a href="zmert.html">Running Z-MERT</a> </li>
          <li> <a href="lattice.html">Lattices</a> </li>
          <li> <a href="server.html">TCP/IP server</a> </li>
          <li> <a href="bundle.html">Bundled configuration</a> </li>
        </ul>
      </div>

      <div class="infobox">
        <b>Help</b><br />
        <ul>
          <li> <a href="faq.html">Answers</a> </li>
          <li> <a href="https://groups.google.com/d/forum/joshua_support">Archive</a> </li>
        </ul>
      </div>

      <div class="footer">
        Last updated on April 08, 2016
      </div>

    </div>

    <div id="main">
      <div id="title">
        <h1>Building large LMs with SRILM</h1>
      </div>

      <div id="content">
        
        <p>The following is a tutorial for building a large language model from the
English Gigaword Fifth Edition corpus
<a href="http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2011T07">LDC2011T07</a>
using SRILM. English text is provided from seven different sources.</p>

<h3 id="step-0-clean-up-the-corpus">Step 0: Clean up the corpus</h3>

<p>The Gigaword corpus has to be stripped of all SGML tags and tokenized.
Instructions for performing those steps are not included in this
documentation. A description of this process can be found in a paper
called <a href="https://akbcwekex2012.files.wordpress.com/2012/05/28_paper.pdf">“Annotated
Gigaword”</a>.</p>

<p>The Joshua package ships with a script that converts all alphabetical
characters to their lowercase equivalent. The script is located at
<code class="highlighter-rouge">$JOSHUA/scripts/lowercase.perl</code>.</p>

<p>Make a directory structure as follows:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>gigaword/
├── corpus/
│   ├── afp_eng/
│   │   ├── afp_eng_199405.lc.gz
│   │   ├── afp_eng_199406.lc.gz
│   │   ├── ...
│   │   └── counts/
│   ├── apw_eng/
│   │   ├── apw_eng_199411.lc.gz
│   │   ├── apw_eng_199412.lc.gz
│   │   ├── ...
│   │   └── counts/
│   ├── cna_eng/
│   │   ├── ...
│   │   └── counts/
│   ├── ltw_eng/
│   │   ├── ...
│   │   └── counts/
│   ├── nyt_eng/
│   │   ├── ...
│   │   └── counts/
│   ├── wpb_eng/
│   │   ├── ...
│   │   └── counts/
│   └── xin_eng/
│       ├── ...
│       └── counts/
└── lm/
    ├── afp_eng/
    ├── apw_eng/
    ├── cna_eng/
    ├── ltw_eng/
    ├── nyt_eng/
    ├── wpb_eng/
    └── xin_eng/
</code></pre>
</div>

<p>The next step will be to build smaller LMs and then interpolate them into one
file.</p>

<h3 id="step-1-count-ngrams">Step 1: Count ngrams</h3>

<p>Run the following script once from each source directory under the <code class="highlighter-rouge">corpus/</code>
directory (edit it to specify the path to the <code class="highlighter-rouge">ngram-count</code> binary as well as
the number of processors):</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/bin/sh</span>

<span class="nv">NGRAM_COUNT</span><span class="o">=</span><span class="nv">$SRILM_SRC</span>/bin/i686-m64/ngram-count
<span class="nv">args</span><span class="o">=</span><span class="s2">""</span>

<span class="k">for </span><span class="nb">source </span><span class="k">in</span> <span class="k">*</span>.gz; <span class="k">do
   </span><span class="nv">args</span><span class="o">=</span><span class="nv">$args</span><span class="s2">"-sort -order 5 -text </span><span class="nv">$source</span><span class="s2"> -write counts/</span><span class="nv">$source</span><span class="s2">-counts.gz "</span>
<span class="k">done

</span><span class="nb">echo</span> <span class="nv">$args</span> | xargs --max-procs<span class="o">=</span>4 -n 7 <span class="nv">$NGRAM_COUNT</span>
</code></pre>
</div>

<p>Then move each <code class="highlighter-rouge">counts/</code> directory to the corresponding directory under
<code class="highlighter-rouge">lm/</code>. Now that each ngram has been counted, we can make a language
model for each of the seven sources.</p>

<h3 id="step-2-make-individual-language-models">Step 2: Make individual language models</h3>

<p>SRILM includes a script, called <code class="highlighter-rouge">make-big-lm</code>, for building large language
models under resource-limited environments. The manual for this script can be
read online
<a href="http://www-speech.sri.com/projects/srilm/manpages/training-scripts.1.html">here</a>.
Since the Gigaword corpus is so large, it is convenient to use <code class="highlighter-rouge">make-big-lm</code>
even in environments with many parallel processors and a lot of memory.</p>

<p>Initiate the following script from each of the source directories under the
<code class="highlighter-rouge">lm/</code> directory (edit it to specify the path to the <code class="highlighter-rouge">make-big-lm</code> script as
well as the pruning threshold):</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="nb">set</span> -x

<span class="nv">CMD</span><span class="o">=</span><span class="nv">$SRILM_SRC</span>/bin/make-big-lm
<span class="nv">PRUNE_THRESHOLD</span><span class="o">=</span>1e-8

<span class="nv">$CMD</span> <span class="se">\</span>
  -name gigalm <span class="sb">`</span><span class="k">for </span>k <span class="k">in </span>counts/<span class="k">*</span>.gz; <span class="k">do </span><span class="nb">echo</span> <span class="s2">" </span><span class="se">\</span><span class="s2">
  -read </span><span class="nv">$k</span><span class="s2"> "</span>; <span class="k">done</span><span class="sb">`</span> <span class="se">\</span>
  -lm lm.gz <span class="se">\</span>
  -max-per-file 100000000 <span class="se">\</span>
  -order 5 <span class="se">\</span>
  -kndiscount <span class="se">\</span>
  -interpolate <span class="se">\</span>
  -unk <span class="se">\</span>
  -prune <span class="nv">$PRUNE_THRESHOLD</span>
</code></pre>
</div>

<p>The language model attributes chosen are the following:</p>

<ul>
  <li>N-grams up to order 5</li>
  <li>Kneser-Ney smoothing</li>
  <li>N-gram probability estimates at the specified order <em>n</em> are interpolated with
lower-order estimates</li>
  <li>include the unknown-word token as a regular word</li>
  <li>pruning N-grams based on the specified threshold</li>
</ul>

<p>Next, we will mix the models together into a single file.</p>

<h3 id="step-3-mix-models-together">Step 3: Mix models together</h3>

<p>Using development text, interpolation weights can determined that give highest
weight to the source language models that have the lowest perplexity on the
specified development set.</p>

<h4 id="step-3-1-determine-interpolation-weights">Step 3-1: Determine interpolation weights</h4>

<p>Initiate the following script from the <code class="highlighter-rouge">lm/</code> directory (edit it to specify the
path to the <code class="highlighter-rouge">ngram</code> binary as well as the path to the development text file):</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="nb">set</span> -x

<span class="nv">NGRAM</span><span class="o">=</span><span class="nv">$SRILM_SRC</span>/bin/i686-m64/ngram
<span class="nv">DEV_TEXT</span><span class="o">=</span>~mpost/expts/wmt12/runs/es-en/data/tune/tune.tok.lc.es

<span class="nb">dirs</span><span class="o">=(</span> afp_eng apw_eng cna_eng ltw_eng nyt_eng wpb_eng xin_eng <span class="o">)</span>

<span class="k">for </span>d <span class="k">in</span> <span class="k">${</span><span class="nv">dirs</span><span class="p">[@]</span><span class="k">}</span> ; <span class="k">do</span>
  <span class="nv">$NGRAM</span> -debug 2 -order 5 -unk -lm <span class="nv">$d</span>/lm.gz -ppl <span class="nv">$DEV_TEXT</span> &gt; <span class="nv">$d</span>/lm.ppl ;
<span class="k">done

</span>compute-best-mix <span class="k">*</span>/lm.ppl &gt; best-mix.ppl
</code></pre>
</div>

<p>Take a look at the contents of <code class="highlighter-rouge">best-mix.ppl</code>. It will contain a sequence of
values in parenthesis. These are the interpolation weights of the source
language models in the order specified. Copy and paste the values within the
parenthesis into the script below.</p>

<h4 id="step-3-2-combine-the-models">Step 3-2: Combine the models</h4>

<p>Initiate the following script from the <code class="highlighter-rouge">lm/</code> directory (edit it to specify the
path to the <code class="highlighter-rouge">ngram</code> binary as well as the interpolation weights):</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="nb">set</span> -x

<span class="nv">NGRAM</span><span class="o">=</span><span class="nv">$SRILM_SRC</span>/bin/i686-m64/ngram
<span class="nv">DIRS</span><span class="o">=(</span>   afp_eng    apw_eng     cna_eng  ltw_eng   nyt_eng  wpb_eng  xin_eng <span class="o">)</span>
<span class="nv">LAMBDAS</span><span class="o">=(</span>0.00631272 0.000647602 0.251555 0.0134726 0.348953 0.371566 0.00749238<span class="o">)</span>

<span class="nv">$NGRAM</span> -order 5 -unk <span class="se">\</span>
  -lm      <span class="k">${</span><span class="nv">DIRS</span><span class="p">[0]</span><span class="k">}</span>/lm.gz     -lambda  <span class="k">${</span><span class="nv">LAMBDAS</span><span class="p">[0]</span><span class="k">}</span> <span class="se">\</span>
  -mix-lm  <span class="k">${</span><span class="nv">DIRS</span><span class="p">[1]</span><span class="k">}</span>/lm.gz <span class="se">\</span>
  -mix-lm2 <span class="k">${</span><span class="nv">DIRS</span><span class="p">[2]</span><span class="k">}</span>/lm.gz -mix-lambda2 <span class="k">${</span><span class="nv">LAMBDAS</span><span class="p">[2]</span><span class="k">}</span> <span class="se">\</span>
  -mix-lm3 <span class="k">${</span><span class="nv">DIRS</span><span class="p">[3]</span><span class="k">}</span>/lm.gz -mix-lambda3 <span class="k">${</span><span class="nv">LAMBDAS</span><span class="p">[3]</span><span class="k">}</span> <span class="se">\</span>
  -mix-lm4 <span class="k">${</span><span class="nv">DIRS</span><span class="p">[4]</span><span class="k">}</span>/lm.gz -mix-lambda4 <span class="k">${</span><span class="nv">LAMBDAS</span><span class="p">[4]</span><span class="k">}</span> <span class="se">\</span>
  -mix-lm5 <span class="k">${</span><span class="nv">DIRS</span><span class="p">[5]</span><span class="k">}</span>/lm.gz -mix-lambda5 <span class="k">${</span><span class="nv">LAMBDAS</span><span class="p">[5]</span><span class="k">}</span> <span class="se">\</span>
  -mix-lm6 <span class="k">${</span><span class="nv">DIRS</span><span class="p">[6]</span><span class="k">}</span>/lm.gz -mix-lambda6 <span class="k">${</span><span class="nv">LAMBDAS</span><span class="p">[6]</span><span class="k">}</span> <span class="se">\</span>
  -write-lm mixed_lm.gz
</code></pre>
</div>

<p>The resulting file, <code class="highlighter-rouge">mixed_lm.gz</code> is a language model based on all the text in
the Gigaword corpus and with some probabilities biased to the development text
specify in step 3-1. It is in the ARPA format. The optional next step converts
it into KenLM format.</p>

<h4 id="step-3-3-convert-to-kenlm">Step 3-3: Convert to KenLM</h4>

<p>The KenLM format has some speed advantages over the ARPA format. Issuing the
following command will write a new language model file <code class="highlighter-rouge">mixed_lm-kenlm.gz</code> that
is the <code class="highlighter-rouge">mixed_lm.gz</code> language model transformed into the KenLM format.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>$JOSHUA/src/joshua/decoder/ff/lm/kenlm/build_binary mixed_lm.gz mixed_lm.kenlm
</code></pre>
</div>



      </div>
    </div>

  </body>
</html>





